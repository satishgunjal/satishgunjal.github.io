---
title: 'Blog Post number 1'
date: 2012-08-14
permalink: /posts/2012/08/blog-post-1/
tags:
  - cool posts
  - category1
  - category2
---


### **Gradient Descent Algorithm**
This is heart of our model, gradient descent algorithm will help us to find optimum values of theta parameters. Inner working of gradient descent algoithm is as below,
  - We start with random value of $θ_0$ and $θ_1$
  - Calculate the cost using cost function
  - Change the value of $θ_0$ and $θ_1$ in order to find minimum cost value
  - Keep doing this till we get the minimum value of cost

In order to change the value of theta its important to know whether to increase or decrease its value and by how much margin. Remember our cost function is a convex function and our objective is to go to its bottom. 
Partial derivative of the cost function will give us the slope at that point.

Consider below examples of positive and negatives slopes,

In case of positive slope we have to decrease the value of $θ_1$ to get minimum cost value.

![Positive Slope](https://raw.githubusercontent.com/satishgunjal/Images/master/Positive_Slope.png)

In case of negative slope we have to increase the value of $θ_1$ to get minimum cost value.

![Negative Slope](https://raw.githubusercontent.com/satishgunjal/Images/master/Negative_Slope.png)

So with the help of slope, we can decide whether to increae or decrease the theta value, but to control the mangitude of change we are going to use **'Learning Parameter Alpha (α)'**.

so the final formla to change the theta value is as below,

**$θ_0$ = $θ_0$ - alpha * partial derivative of cost function w.r.t $θ_0$**

**$θ_1$ = $θ_1$ - alpha * partial derivative of cost function w.r.t $θ_1$**

After replacing the value of partial derivative of cost function, our formula to get theta values will looks like,

![Gradient Descent Formula](https://raw.githubusercontent.com/satishgunjal/Images/master/Gradient_Descent_Formula.png)

Since at every step of gredient discent we are calculating the cost using all the training example, it is also called as **'Batch Gradient Descent'** algirithm

Enough of theory, now lets implement gradient descent algorithm using Python and create our linear model 
